{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pR7wq97EtSE"
   },
   "source": [
    "# PyTorch Metric Learning\n",
    "### Example for the TrainWithClassifier trainer\n",
    "See the documentation [here](https://kevinmusgrave.github.io/pytorch-metric-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKpRHvy24tV7"
   },
   "source": [
    "## Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeIGxbbp3W2S"
   },
   "outputs": [],
   "source": [
    "!pip install -q pytorch-metric-learning[with-hooks]\n",
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfqRRbIw4zYR"
   },
   "source": [
    "## Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "567qnmi7wk_M",
    "outputId": "7b43cec6-243e-44bf-89f5-00f2d72d508b"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import record_keeper\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import umap\n",
    "from cycler import cycler\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pytorch_metric_learning\n",
    "import pytorch_metric_learning.utils.logging_presets as logging_presets\n",
    "from pytorch_metric_learning import losses, miners, samplers, testers, trainers\n",
    "from pytorch_metric_learning.utils import common_functions\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.info(\"VERSION %s\" % pytorch_metric_learning.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qxs6EEeR496q"
   },
   "source": [
    "## Simple model def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKyR6gnTwk_P"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # layer_sizes[0] is the dimension of the input\n",
    "    # layer_sizes[-1] is the dimension of the output\n",
    "    def __init__(self, layer_sizes, final_relu=False):\n",
    "        super().__init__()\n",
    "        layer_list = []\n",
    "        layer_sizes = [int(x) for x in layer_sizes]\n",
    "        num_layers = len(layer_sizes) - 1\n",
    "        final_relu_layer = num_layers if final_relu else num_layers - 1\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            input_size = layer_sizes[i]\n",
    "            curr_size = layer_sizes[i + 1]\n",
    "            if i < final_relu_layer:\n",
    "                layer_list.append(nn.ReLU(inplace=False))\n",
    "            layer_list.append(nn.Linear(input_size, curr_size))\n",
    "        self.net = nn.Sequential(*layer_list)\n",
    "        self.last_linear = self.net[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btjxk6zR5Cl6"
   },
   "source": [
    "## Initialize models, optimizers and image transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tzmyFS3wk_R"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set trunk model and replace the softmax layer with an identity function\n",
    "trunk = torchvision.models.resnet18(pretrained=True)\n",
    "trunk_output_size = trunk.fc.in_features\n",
    "trunk.fc = common_functions.Identity()\n",
    "trunk = torch.nn.DataParallel(trunk.to(device))\n",
    "\n",
    "# Set embedder model. This takes in the output of the trunk and outputs 64 dimensional embeddings\n",
    "embedder = torch.nn.DataParallel(MLP([trunk_output_size, 64]).to(device))\n",
    "\n",
    "# Set the classifier. The classifier will take the embeddings and output a 50 dimensional vector.\n",
    "# (Our training set will consist of the first 50 classes of the CIFAR100 dataset.)\n",
    "# We'll specify the classification loss further down in the code.\n",
    "classifier = torch.nn.DataParallel(MLP([64, 50])).to(device)\n",
    "\n",
    "# Set optimizers\n",
    "trunk_optimizer = torch.optim.Adam(trunk.parameters(), lr=0.00001, weight_decay=0.0001)\n",
    "embedder_optimizer = torch.optim.Adam(\n",
    "    embedder.parameters(), lr=0.0001, weight_decay=0.0001\n",
    ")\n",
    "classifier_optimizer = torch.optim.Adam(\n",
    "    classifier.parameters(), lr=0.0001, weight_decay=0.0001\n",
    ")\n",
    "\n",
    "# Set the image transforms\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(64),\n",
    "        transforms.RandomResizedCrop(scale=(0.16, 1), ratio=(0.75, 1.33), size=64),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(64),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf0xgdWS5GqG"
   },
   "source": [
    "## Create the dataset and class-disjoint train/val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "D-nmnYYAwk_T",
    "outputId": "45f2f29c-2184-4741-a327-d5be9c962791"
   },
   "outputs": [],
   "source": [
    "# Download the original datasets\n",
    "original_train = datasets.CIFAR100(\n",
    "    root=\"CIFAR100_Dataset\", train=True, transform=None, download=True\n",
    ")\n",
    "original_val = datasets.CIFAR100(\n",
    "    root=\"CIFAR100_Dataset\", train=False, transform=None, download=True\n",
    ")\n",
    "\n",
    "# This will be used to create train and val sets that are class-disjoint\n",
    "class ClassDisjointCIFAR100(torch.utils.data.Dataset):\n",
    "    def __init__(self, original_train, original_val, train, transform):\n",
    "        rule = (lambda x: x < 50) if train else (lambda x: x >= 50)\n",
    "        train_filtered_idx = [\n",
    "            i for i, x in enumerate(original_train.targets) if rule(x)\n",
    "        ]\n",
    "        val_filtered_idx = [i for i, x in enumerate(original_val.targets) if rule(x)]\n",
    "        self.data = np.concatenate(\n",
    "            [\n",
    "                original_train.data[train_filtered_idx],\n",
    "                original_val.data[val_filtered_idx],\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "        self.targets = np.concatenate(\n",
    "            [\n",
    "                np.array(original_train.targets)[train_filtered_idx],\n",
    "                np.array(original_val.targets)[val_filtered_idx],\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "# Class disjoint training and validation set\n",
    "train_dataset = ClassDisjointCIFAR100(\n",
    "    original_train, original_val, True, train_transform\n",
    ")\n",
    "val_dataset = ClassDisjointCIFAR100(original_train, original_val, False, val_transform)\n",
    "assert set(train_dataset.targets).isdisjoint(set(val_dataset.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7J817Vs5LNs"
   },
   "source": [
    "##Create the loss, miner, sampler, and package them into dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kp9AC_4Dwk_V",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the loss function\n",
    "loss = losses.TripletMarginLoss(margin=0.1)\n",
    "\n",
    "# Set the classification loss:\n",
    "classification_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the mining function\n",
    "miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
    "\n",
    "# Set the dataloader sampler\n",
    "sampler = samplers.MPerClassSampler(\n",
    "    train_dataset.targets, m=4, length_before_new_iter=len(train_dataset)\n",
    ")\n",
    "\n",
    "# Set other training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 4\n",
    "\n",
    "# Package the above stuff into dictionaries.\n",
    "models = {\"trunk\": trunk, \"embedder\": embedder, \"classifier\": classifier}\n",
    "optimizers = {\n",
    "    \"trunk_optimizer\": trunk_optimizer,\n",
    "    \"embedder_optimizer\": embedder_optimizer,\n",
    "    \"classifier_optimizer\": classifier_optimizer,\n",
    "}\n",
    "loss_funcs = {\"metric_loss\": loss, \"classifier_loss\": classification_loss}\n",
    "mining_funcs = {\"tuple_miner\": miner}\n",
    "\n",
    "# We can specify loss weights if we want to. This is optional\n",
    "loss_weights = {\"metric_loss\": 1, \"classifier_loss\": 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXMP1gI1Sdam"
   },
   "outputs": [],
   "source": [
    "# Remove logs if you want to train with new parameters\n",
    "!rm -rf example_logs/ example_saved_models/ example_tensorboard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9fa1VYD5Yv0"
   },
   "source": [
    "## Create the training and testing hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vq_Pd7Pd5Xi_"
   },
   "outputs": [],
   "source": [
    "record_keeper, _, _ = logging_presets.get_record_keeper(\n",
    "    \"example_logs\", \"example_tensorboard\"\n",
    ")\n",
    "hooks = logging_presets.get_hook_container(record_keeper)\n",
    "dataset_dict = {\"val\": val_dataset}\n",
    "model_folder = \"example_saved_models\"\n",
    "\n",
    "\n",
    "def visualizer_hook(umapper, umap_embeddings, labels, split_name, keyname, *args):\n",
    "    logging.info(\n",
    "        \"UMAP plot for the {} split and label set {}\".format(split_name, keyname)\n",
    "    )\n",
    "    label_set = np.unique(labels)\n",
    "    num_classes = len(label_set)\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    plt.gca().set_prop_cycle(\n",
    "        cycler(\n",
    "            \"color\", [plt.cm.nipy_spectral(i) for i in np.linspace(0, 0.9, num_classes)]\n",
    "        )\n",
    "    )\n",
    "    for i in range(num_classes):\n",
    "        idx = labels == label_set[i]\n",
    "        plt.plot(umap_embeddings[idx, 0], umap_embeddings[idx, 1], \".\", markersize=1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create the tester\n",
    "tester = testers.GlobalEmbeddingSpaceTester(\n",
    "    end_of_testing_hook=hooks.end_of_testing_hook,\n",
    "    visualizer=umap.UMAP(),\n",
    "    visualizer_hook=visualizer_hook,\n",
    "    dataloader_num_workers=2,\n",
    "    accuracy_calculator=AccuracyCalculator(k=\"max_bin_count\"),\n",
    ")\n",
    "\n",
    "end_of_epoch_hook = hooks.end_of_epoch_hook(\n",
    "    tester, dataset_dict, model_folder, test_interval=1, patience=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0D3Jvxc5iWD"
   },
   "source": [
    "## Create the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuASrVs-wk_X",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = trainers.TrainWithClassifier(\n",
    "    models,\n",
    "    optimizers,\n",
    "    batch_size,\n",
    "    loss_funcs,\n",
    "    mining_funcs,\n",
    "    train_dataset,\n",
    "    sampler=sampler,\n",
    "    dataloader_num_workers=2,\n",
    "    loss_weights=loss_weights,\n",
    "    end_of_iteration_hook=hooks.end_of_iteration_hook,\n",
    "    end_of_epoch_hook=end_of_epoch_hook,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIlMMUxSPLni"
   },
   "source": [
    "## Start Tensorboard\n",
    "(Turn off adblock and other shields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikIOmWNNPLtg"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir example_tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIq7s7jf5ksj"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WHza2JJHwk_Z",
    "outputId": "95f52b2c-712b-4be2-c33c-d59de81df50b"
   },
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TrainWithClassifier.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
