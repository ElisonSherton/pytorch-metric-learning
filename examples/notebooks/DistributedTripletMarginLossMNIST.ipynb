{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SEqLaxOq7kw",
    "outputId": "eaca39d5-94ac-4e33-89a1-f97b30c66a27"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-metric-learning\n",
    "!pip install faiss-cpu # we're using cpu for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vYmfxuw1tYdm",
    "outputId": "b414f826-84e6-4eb9-b80d-e9f406dfded1"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "######################################################################################################################\n",
    "### This script is modified from the guide on pytorch distributed training https://github.com/seba-1511/dist_tuto.pth/\n",
    "### https://pytorch.org/tutorials/intermediate/dist_tuto.html\n",
    "######################################################################################################################\n",
    "import os\n",
    "from math import ceil\n",
    "from random import Random\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.multiprocessing import Process\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pytorch_metric_learning import losses, miners, testers\n",
    "from pytorch_metric_learning.utils import distributed as pml_dist\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class Partition(object):\n",
    "    \"\"\"Dataset-like object, but only access a subset of it.\"\"\"\n",
    "\n",
    "    def __init__(self, data, index):\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_idx = self.index[index]\n",
    "        return self.data[data_idx]\n",
    "\n",
    "\n",
    "class DataPartitioner(object):\n",
    "    \"\"\"Partitions a dataset into different chuncks.\"\"\"\n",
    "\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        data_len = len(data)\n",
    "        indexes = [x for x in range(0, data_len)]\n",
    "        rng.shuffle(indexes)\n",
    "\n",
    "        for frac in sizes:\n",
    "            part_len = int(frac * data_len)\n",
    "            self.partitions.append(indexes[0:part_len])\n",
    "            indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Network architecture.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        return self.fc1(x)\n",
    "\n",
    "\n",
    "def get_MNIST(train):\n",
    "    return datasets.MNIST(\n",
    "        \"./data\",\n",
    "        train=train,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def partition_dataset(dataset):\n",
    "    \"\"\"Partitioning MNIST\"\"\"\n",
    "    size = dist.get_world_size()\n",
    "    bsz = 512 // size\n",
    "    partition_sizes = [1.0 / size for _ in range(size)]\n",
    "    partition = DataPartitioner(dataset, partition_sizes)\n",
    "    partition = partition.use(dist.get_rank())\n",
    "    train_set = torch.utils.data.DataLoader(partition, batch_size=bsz, shuffle=True)\n",
    "    return train_set, bsz\n",
    "\n",
    "\n",
    "### convenient function from pytorch-metric-learning ###\n",
    "def get_all_embeddings(dataset, model, data_device):\n",
    "    # dataloader_num_workers has to be 0 to avoid pid error\n",
    "    # This only happens when within multiprocessing\n",
    "    tester = testers.BaseTester(dataloader_num_workers=0, data_device=data_device)\n",
    "    return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator, data_device):\n",
    "    train_embeddings, train_labels = get_all_embeddings(train_set, model, data_device)\n",
    "    test_embeddings, test_labels = get_all_embeddings(test_set, model, data_device)\n",
    "    print(\"Computing accuracy\")\n",
    "    accuracies = accuracy_calculator.get_accuracy(\n",
    "        test_embeddings, train_embeddings, test_labels, train_labels, False\n",
    "    )\n",
    "    print(\n",
    "        \"Validation set accuracy (Precision@1) = {}\".format(\n",
    "            accuracies[\"precision_at_1\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def test_model(rank, train_set, test_set, model, epoch, data_device):\n",
    "    if rank == 0:\n",
    "        print(\"Computing validation set accuracy for epoch {}\".format(epoch))\n",
    "        accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)\n",
    "        test(train_set, test_set, model, accuracy_calculator, data_device)\n",
    "    dist.barrier()\n",
    "\n",
    "\n",
    "def run(rank, size, train_dataset, val_dataset):\n",
    "    \"\"\"Distributed Synchronous SGD Example\"\"\"\n",
    "    print(\"Rank {} entering the 'run' function\".format(rank))\n",
    "    torch.manual_seed(1234)\n",
    "    train_set, bsz = partition_dataset(train_dataset)\n",
    "    dist.barrier()\n",
    "    ### use this if you have multiple GPUs ###\n",
    "    # device = torch.device(\"cuda:{}\".format(rank))\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = Net()\n",
    "    ### if you have multiple GPUs, set this to DDP(model.to(device), device_ids=[rank])\n",
    "    model = DDP(model.to(device))\n",
    "    test_model(rank, train_dataset, val_dataset, model, \"untrained\", device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    #####################################\n",
    "    ### pytorch-metric-learning stuff ###\n",
    "    loss_fn = losses.TripletMarginLoss()\n",
    "    loss_fn = pml_dist.DistributedLossWrapper(loss=loss_fn, efficient=True)\n",
    "    miner = miners.MultiSimilarityMiner()\n",
    "    miner = pml_dist.DistributedMinerWrapper(miner=miner, efficient=True)\n",
    "    ### pytorch-metric-learning stuff ###\n",
    "    #####################################\n",
    "\n",
    "    num_batches = ceil(len(train_set.dataset) / float(bsz))\n",
    "    for epoch in range(1):\n",
    "        epoch_loss = 0.0\n",
    "        print(\"Rank {} starting epoch {}\".format(rank, epoch))\n",
    "        for i, (data, target) in enumerate(train_set):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            hard_pairs = miner(output, target)\n",
    "            loss = loss_fn(output, target, hard_pairs)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 10 == 0:\n",
    "                print(\n",
    "                    \"Rank {}, iteration {}, loss {}, num pos pairs {}, num neg pairs {}\".format(\n",
    "                        rank,\n",
    "                        i,\n",
    "                        loss.item(),\n",
    "                        miner.miner.num_pos_pairs,\n",
    "                        miner.miner.num_neg_pairs,\n",
    "                    )\n",
    "                )\n",
    "            dist.barrier()\n",
    "\n",
    "        print(\n",
    "            \"Rank {}, epoch {}, average loss {}\".format(\n",
    "                rank, epoch, epoch_loss / num_batches\n",
    "            )\n",
    "        )\n",
    "        test_model(rank, train_dataset, val_dataset, model, epoch, device)\n",
    "\n",
    "\n",
    "#######################################\n",
    "### Set backend='nccl' if using GPU ###\n",
    "#######################################\n",
    "def init_processes(rank, size, fn, train_dataset, val_dataset, backend=\"gloo\"):\n",
    "    \"\"\"Initialize the distributed environment.\"\"\"\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size, train_dataset, val_dataset)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset = get_MNIST(True)\n",
    "    val_dataset = get_MNIST(False)\n",
    "\n",
    "    size = 4\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(\n",
    "            target=init_processes, args=(rank, size, run, train_dataset, val_dataset)\n",
    "        )\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DistributedTripletMarginLossMNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}